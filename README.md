# TRANSFORMER-INTRO
***变形金刚？All you need is Attention？Encoder？Decoder？Let's figure it out !***

## 模型结构

<img src=".\figs\structure.png" align="middle" alt="structure" style="zoom:70%;" />



```
模型分为`Encoder`(编码器)和`Decoder`(解码器)两个部分，分别对应上图中的左右两部分。

其中编码器由N个相同的层堆叠在一起(后面的代码取N=6)，每一层又有两个子层：

第一个子层是一个`Multi-Head Attention`(多头的自注意机制)，

第二个子层是一个简单的`Feed Forward`(全连接前馈网络)。

两个子层都添加了一个残差连接（Add） + layer normalization（Norm）的操作。

模型的解码器同样是堆叠了N个相同的层，不过和编码器中每层的结构稍有不同。

对于解码器的每一层，除了编码器中的两个子层`Multi-Head Attention`和`Feed Forward`，

解码器还包含一个子层`Masked Multi-Head Attention`，

如图中所示每个子层同样也用了residual以及layer normalization。

模型的输入由`Input Embedding`和`Positional Encoding`(位置编码)两部分组合而成，

模型的输出由Decoder的输出简单的经过softmax得到。
```



## 1.Embeeding层

```
Embedding层的作用是将某种格式的输入数据，

例如文本，转变为模型可以处理的向量表示，

来描述原始数据所包含的信息。

Embedding层输出的可以理解为当前时间步的特征，

如果是文本任务，这里就可以是Word Embedding，

如果是其他任务，就可以是任何合理方法所提取的特征。

核心是借助torch提供的nn.Embedding
```

## 2.位置编码

```
位置编码 PositionalEncoding

Positional Encodding位置编码的作用是为模型提供当前时间步的前后出现顺序的信息。

因为Transformer不像RNN那样的循环结构有前后不同时间步输入间天然的先后顺序，

所有的时间步是同时输入，并行推理的，因此在时间步的特征中融合进位置编码的信息是合理的。

位置编码可以有很多选择，可以是固定的，也可以设置成可学习的参数。

我们使用固定的位置编码,使用不同频率的sin和cos函数来进行位置编码
```

<img src=".\figs\gongshi.png" align="middle" alt="gongshi" style="zoom:75%;" />

```
可以认为，最终模型的输入是若干个时间步对应的embedding，

每一个时间步对应一个embedding，可以理解为是当前时间步的一个综合的特征信息，

即包含了本身的语义信息，又包含了当前时间步在整个句子中的位置信息
```

## 3.编码器

```
编码器作用是用于对输入进行特征提取，为解码环节提供有效的语义信息
```

## 4.编码器层

```
每个编码器层由两个子层连接结构组成
第一个子层包括一个多头自注意力层和规范化层以及一个残差连接
第二个子层包括一个前馈全连接层和规范化层以及一个残差连接
```

## 5.注意力机制

```
人类在观察事物时，无法同时仔细观察眼前的一切，只能聚焦到某一个局部。

通常我们大脑在简单了解眼前的场景后，能够很快把注意力聚焦到最有价值的局部来仔细观察，

从而作出有效判断。或许是基于这样的启发，想到了在算法中利用注意力机制。

注意力计算：它需要三个指定的输入Q（query），K（key），V（value），然后通过下面公式得到注意力的计算结果。
```
<img src=".\figs\gongshi2.png" align="middle" alt="structure"  />

<img src=".\figs\4.png" align="middle" alt="structure"  />

```
当前时间步的注意力计算结果，是一个组系数 * 每个时间步的特征向量value的累加，

而这个系数，通过当前时间步的query和其他时间步对应的key做内积得到，

这个过程相当于用自己的query对别的时间步的key做查询，判断相似度，

决定以多大的比例将对应时间步的信息继承过来
```

## 6.多头注意力机制

```
在搭建EncoderLayer时候所使用的Attention模块，实际使用的是多头注意力，可以简单理解为多个注意力模块组合在一起。

多头注意力机制的作用：这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，

从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元表达，实验表明可以从而提升模型效果。
```

<img src=".\figs\5.png" align="middle" alt="structure"  />

```
例子：bank是银行的意思，如果只有一个注意力模块，那么它大概率会学习去关注类似money、loan贷款这样的词。如果我们使用多个多头机制，那么不同的头就会去关注不同的语义，比如bank还有一种含义是河岸，那么可能有一个头就会去关注类似river这样的词汇
```

## 7.前馈全连接层

```
EncoderLayer中另一个核心的子层是 Feed Forward Layer

在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，

对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出：
```

<img src=".\figs\333333.png" align="middle" alt="structure"  />

```
Feed Forward Layer 其实就是简单的由两个前向全连接层组成，

核心在于，Attention模块每个时间步的输出都整合了所有时间步的信息，

而Feed Forward Layer每个时间步只是对自己的特征的一个进一步整合，与其他时间步无关。
```

## 8.规范化层

```
规范化层的作用：它是所有深层网络模型都需要的标准网络层，

因为随着网络层数的增加，通过多层的计算后输出可能开始出现过大或过小的情况，

这样可能会导致学习过程出现异常，模型可能收敛非常慢。

因此都会在一定层后接规范化层进行数值的规范化，使其特征数值在合理范围内。
```

## 9.掩码及其作用

```
掩码：掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有0和1；代表位置被遮掩或者不被遮掩。

掩码的作用：在transformer中，掩码主要的作用有两个，一个是屏蔽掉无效的padding区域，

一个是屏蔽掉来自“未来”的信息。Encoder中的掩码主要是起到第一个作用，Decoder中的掩码则同时发挥着两种作用。

屏蔽掉无效的padding区域：我们训练需要组batch进行，就以机器翻译任务为例，

一个batch中不同样本的输入长度很可能是不一样的，此时我们要设置一个最大句子长度，

然后对空白区域进行padding填充，而填充的区域无论在Encoder还是Decoder的计算中都是没有意义的，

因此需要用mask进行标识，屏蔽掉对应区域的响应。

屏蔽掉来自未来的信息：我们已经学习了attention的计算流程，它是会综合所有时间步的计算的，

那么在解码的时候，就有可能获取到未来的信息，这是不行的。

因此，这种情况也需要我们使用mask进行屏蔽。
```

## 10.解码器整体结构&解码器层

```
根据编码器的结果以及上一次预测的结果，输出序列的下一个结果。

解码器也是由N个相同层堆叠而成

每个解码器层由三个子层连接结构组成，

第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接，

第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接，

第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接。
```

<img src=".\figs\10.png" align="middle" alt="structure"  />

```
解码器层中的各个子模块，如，多头注意力机制，规范化层，前馈全连接都与编码器中的实现相同。

有一个细节需要注意，

第一个子层的多头注意力和编码器中完全一致，

第二个子层，它的多头注意力模块中，query来自上一个子层，key 和 value 来自编码器的输出。

可以这样理解，就是第二层负责，利用解码器已经预测出的信息作为query，

去编码器提取的各种特征中，查找相关信息并融合到当前特征中，来完成预测。
```

## 11.模型输出

```
线性层的作用：通过对上一步的线性变化得到指定维度的输出，

也就是转换维度的作用。转换后的维度对应着输出类别的个数，

如果是翻译任务，那就对应的是文字字典的大小。
```

<img src=".\figs\11.png" align="middle" alt="structure"  />

